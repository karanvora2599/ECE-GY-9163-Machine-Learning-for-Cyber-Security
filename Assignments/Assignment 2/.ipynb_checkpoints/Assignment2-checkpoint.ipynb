{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3DTItlUYbxt",
        "outputId": "bea96734-fc26-4ac0-ebda-3afed883d5c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lingspam_public']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Define the path to the uploaded file\n",
        "file_path = 'lingspam_public.tar.gz'\n",
        "\n",
        "# Define the extraction directory\n",
        "extraction_dir = 'lingspam_public'\n",
        "\n",
        "# Extract the tar.gz file\n",
        "with tarfile.open(file_path, 'r:gz') as tar:\n",
        "    tar.extractall(path=extraction_dir)\n",
        "\n",
        "# List the contents of the extracted directory\n",
        "extracted_files = os.listdir(extraction_dir)\n",
        "extracted_files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the 'lemm_stop' folder\n",
        "lemm_stop_dir = os.path.join(extraction_dir, 'lingspam_public', 'lemm_stop')\n",
        "\n",
        "# List the contents of the 'lemm_stop' directory\n",
        "lemm_stop_files = os.listdir(lemm_stop_dir)\n",
        "lemm_stop_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ohTOreLYj-_",
        "outputId": "ebba8c68-b1fd-4343-9286-7a9c8562205e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['part10',\n",
              " 'part3',\n",
              " 'part5',\n",
              " 'part6',\n",
              " 'part2',\n",
              " 'part9',\n",
              " 'part4',\n",
              " 'part7',\n",
              " 'part1',\n",
              " 'part8']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import email\n",
        "import re\n",
        "\n",
        "def load_and_preprocess_emails(folder_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess emails from a given folder path.\n",
        "\n",
        "    Args:\n",
        "    folder_path (str): The path to the folder containing email files.\n",
        "\n",
        "    Returns:\n",
        "    list of tuples: A list where each tuple contains the preprocessed email text and its label (0 for ham, 1 for spam).\n",
        "    \"\"\"\n",
        "    emails = []\n",
        "\n",
        "    # List all files in the folder\n",
        "    email_files = os.listdir(folder_path)\n",
        "\n",
        "    for email_file in email_files:\n",
        "        # Define the path to the email file\n",
        "        file_path = os.path.join(folder_path, email_file)\n",
        "\n",
        "        # Read the content of the email file\n",
        "        with open(file_path, 'r', encoding='latin-1') as f:\n",
        "            email_content = f.read()\n",
        "\n",
        "        # Preprocess the email content\n",
        "        # Convert to lowercase and tokenize\n",
        "        tokens = re.findall(r'\\b\\w+\\b', email_content.lower())\n",
        "        preprocessed_email = ' '.join(tokens)\n",
        "\n",
        "        # Label the email (0 for ham, 1 for spam)\n",
        "        label = 1 if email_file.startswith('spmsg') else 0\n",
        "\n",
        "        # Add the preprocessed email and its label to the list\n",
        "        emails.append((preprocessed_email, label))\n",
        "\n",
        "    return emails\n",
        "\n",
        "# Load and preprocess emails from each fold\n",
        "emails_by_fold = {}\n",
        "for fold in lemm_stop_files:\n",
        "    folder_path = os.path.join(lemm_stop_dir, fold)\n",
        "    emails = load_and_preprocess_emails(folder_path)\n",
        "    emails_by_fold[fold] = emails\n",
        "\n",
        "# Display the number of emails loaded from each fold\n",
        "{fold: len(emails) for fold, emails in emails_by_fold.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWLednYYYxPn",
        "outputId": "15619e0f-394e-4b53-e322-ceccd2d9c78f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'part10': 291,\n",
              " 'part3': 289,\n",
              " 'part5': 290,\n",
              " 'part6': 289,\n",
              " 'part2': 289,\n",
              " 'part9': 289,\n",
              " 'part4': 289,\n",
              " 'part7': 289,\n",
              " 'part1': 289,\n",
              " 'part8': 289}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Combine emails from the first 9 folds for training\n",
        "train_emails = [email for fold, emails in list(emails_by_fold.items())[:-1] for email in emails]\n",
        "train_texts, train_labels = zip(*train_emails)\n",
        "\n",
        "# Use the 10th fold for testing\n",
        "test_emails = emails_by_fold['part10']\n",
        "test_texts, test_labels = zip(*test_emails)\n",
        "\n",
        "# Convert to NumPy arrays for easier manipulation later on\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Create a binary CountVectorizer (for Bernoulli NB)\n",
        "binary_vectorizer = CountVectorizer(binary=True)\n",
        "X_train_binary = binary_vectorizer.fit_transform(train_texts)\n",
        "X_test_binary = binary_vectorizer.transform(test_texts)\n",
        "\n",
        "# Create a term frequency CountVectorizer (for Multinomial NB)\n",
        "tf_vectorizer = CountVectorizer(binary=False)\n",
        "X_train_tf = tf_vectorizer.fit_transform(train_texts)\n",
        "X_test_tf = tf_vectorizer.transform(test_texts)\n",
        "\n",
        "# Display the shapes of the term-document matrices\n",
        "X_train_binary.shape, X_train_tf.shape"
      ],
      "metadata": {
        "id": "g5QPsZSNZhKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ead3e22-89b3-48b3-cb33-cd96adca9ccc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2604, 52180), (2604, 52180))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "def select_top_features(X, y, top_k):\n",
        "    \"\"\"\n",
        "    Select the top-k features based on mutual information (information gain).\n",
        "\n",
        "    Args:\n",
        "    X (csr_matrix): The term-document matrix.\n",
        "    y (np.array): The array of labels.\n",
        "    top_k (int): The number of top features to select.\n",
        "\n",
        "    Returns:\n",
        "    csr_matrix: The reduced term-document matrix with only the top-k features.\n",
        "    \"\"\"\n",
        "    # Use SelectKBest with mutual_info_classif to select the top-k features\n",
        "    selector = SelectKBest(score_func=mutual_info_classif, k=top_k)\n",
        "    X_new = selector.fit_transform(X, y)\n",
        "\n",
        "    return X_new, selector.get_support(indices=True)\n",
        "\n",
        "# Select top-10, top-100, and top-1000 features for both binary and TF representations\n",
        "X_train_binary_top10, top10_features_binary = select_top_features(X_train_binary, train_labels, 10)\n",
        "X_train_binary_top100, top100_features_binary = select_top_features(X_train_binary, train_labels, 100)\n",
        "X_train_binary_top1000, top1000_features_binary = select_top_features(X_train_binary, train_labels, 1000)\n",
        "\n",
        "X_train_tf_top10, top10_features_tf = select_top_features(X_train_tf, train_labels, 10)\n",
        "X_train_tf_top100, top100_features_tf = select_top_features(X_train_tf, train_labels, 100)\n",
        "X_train_tf_top1000, top1000_features_tf = select_top_features(X_train_tf, train_labels, 1000)\n",
        "\n",
        "# Display the shapes of the reduced term-document matrices\n",
        "(X_train_binary_top10.shape, X_train_binary_top100.shape, X_train_binary_top1000.shape,\n",
        " X_train_tf_top10.shape, X_train_tf_top100.shape, X_train_tf_top1000.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7y1pNkudOTz",
        "outputId": "a85b8458-58aa-4b96-a5c9-b1ececb19f57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2604, 10), (2604, 100), (2604, 1000), (2604, 10), (2604, 100), (2604, 1000))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import time\n",
        "\n",
        "# Function to train a classifier and evaluate its performance\n",
        "def train_and_evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
        "    start_time = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    y_pred = clf.predict(X_test)\n",
        "    evaluation_time = time.time() - start_time\n",
        "\n",
        "    precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, pos_label=1, average='binary')\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'training_time': training_time,\n",
        "        'evaluation_time': evaluation_time\n",
        "    }\n",
        "\n",
        "# Train and evaluate Bernoulli Naive Bayes with binary features\n",
        "bnb_results = {}\n",
        "\n",
        "# For top-10 features\n",
        "bnb_clf = BernoulliNB(binarize=None)\n",
        "bnb_results['binary_top10'] = train_and_evaluate_classifier(bnb_clf, X_train_binary_top10, train_labels, X_test_binary, test_labels)\n",
        "\n",
        "# For top-100 features\n",
        "bnb_clf = BernoulliNB(binarize=None)\n",
        "bnb_results['binary_top100'] = train_and_evaluate_classifier(bnb_clf, X_train_binary_top100, train_labels, X_test_binary, test_labels)\n",
        "\n",
        "# For top-1000 features\n",
        "bnb_clf = BernoulliNB(binarize=None)\n",
        "bnb_results['binary_top1000'] = train_and_evaluate_classifier(bnb_clf, X_train_binary_top1000, train_labels, X_test_binary, test_labels)\n",
        "\n",
        "bnb_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "PHVLvkXTdRI-",
        "outputId": "f9dcada3-cf56-4f8d-f326-a446e0f8ddd8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f3457ef5c4d5>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# For top-10 features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mbnb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinarize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mbnb_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'binary_top10'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbnb_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_binary_top10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# For top-100 features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f3457ef5c4d5>\u001b[0m in \u001b[0;36mtrain_and_evaluate_classifier\u001b[0;34m(clf, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mevaluation_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m    104\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;34m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinarize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;34m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 52180 features, but BernoulliNB is expecting 10 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "import email\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Define the path to the uploaded file\n",
        "file_path = 'lingspam_public.tar.gz'\n",
        "\n",
        "# Define the extraction directory\n",
        "extraction_dir = 'lingspam_public'\n",
        "\n",
        "# Extract the tar.gz file\n",
        "with tarfile.open(file_path, 'r:gz') as tar:\n",
        "    tar.extractall(path=extraction_dir)\n",
        "\n",
        "# Define the path to the 'lemm_stop' folder\n",
        "lemm_stop_dir = os.path.join(extraction_dir, 'lingspam_public', 'lemm_stop')\n",
        "\n",
        "# Function to load and preprocess emails\n",
        "def load_and_preprocess_emails(folder_path):\n",
        "    emails = []\n",
        "    email_files = os.listdir(folder_path)\n",
        "    for email_file in email_files:\n",
        "        file_path = os.path.join(folder_path, email_file)\n",
        "        with open(file_path, 'r', encoding='latin-1') as f:\n",
        "            email_content = f.read()\n",
        "        tokens = re.findall(r'\\b\\w+\\b', email_content.lower())\n",
        "        preprocessed_email = ' '.join(tokens)\n",
        "        label = 1 if email_file.startswith('spmsg') else 0\n",
        "        emails.append((preprocessed_email, label))\n",
        "    return emails\n",
        "\n",
        "# Load and preprocess emails from each fold\n",
        "emails_by_fold = {}\n",
        "for fold in os.listdir(lemm_stop_dir):\n",
        "    folder_path = os.path.join(lemm_stop_dir, fold)\n",
        "    emails = load_and_preprocess_emails(folder_path)\n",
        "    emails_by_fold[fold] = emails\n",
        "\n",
        "# Combine emails from the first 9 folds for training\n",
        "train_emails = [email for fold, emails in list(emails_by_fold.items())[:-1] for email in emails]\n",
        "train_texts, train_labels = zip(*train_emails)\n",
        "\n",
        "# Use the 10th fold for testing\n",
        "test_emails = emails_by_fold['part10']\n",
        "test_texts, test_labels = zip(*test_emails)\n",
        "\n",
        "# Convert to NumPy arrays for easier manipulation later on\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Create term-document matrices\n",
        "binary_vectorizer = CountVectorizer(binary=True)\n",
        "X_train_binary = binary_vectorizer.fit_transform(train_texts)\n",
        "X_test_binary = binary_vectorizer.transform(test_texts)\n",
        "\n",
        "tf_vectorizer = CountVectorizer(binary=False)\n",
        "X_train_tf = tf_vectorizer.fit_transform(train_texts)\n",
        "X_test_tf = tf_vectorizer.transform(test_texts)\n",
        "\n",
        "X_train_binary.shape, X_train_tf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu3GUgKUgKDX",
        "outputId": "64da0d53-0c42-45cb-f59b-67130d594ef1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2604, 52180), (2604, 52180))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "# Function to select top-N features based on mutual information\n",
        "def select_top_features(X, y, top_k):\n",
        "    selector = SelectKBest(score_func=mutual_info_classif, k=top_k)\n",
        "    X_new = selector.fit_transform(X, y)\n",
        "    return X_new, selector.get_support(indices=True)\n",
        "\n",
        "# Select top-10, top-100, and top-1000 features for binary features\n",
        "X_train_binary_top10, top10_features_binary = select_top_features(X_train_binary, train_labels, 10)\n",
        "X_train_binary_top100, top100_features_binary = select_top_features(X_train_binary, train_labels, 100)\n",
        "X_train_binary_top1000, top1000_features_binary = select_top_features(X_train_binary, train_labels, 1000)\n",
        "\n",
        "# Select top-10, top-100, and top-1000 features for TF features\n",
        "X_train_tf_top10, top10_features_tf = select_top_features(X_train_tf, train_labels, 10)\n",
        "X_train_tf_top100, top100_features_tf = select_top_features(X_train_tf, train_labels, 100)\n",
        "X_train_tf_top1000, top1000_features_tf = select_top_features(X_train_tf, train_labels, 1000)\n",
        "\n",
        "(X_train_binary_top10.shape, X_train_binary_top100.shape, X_train_binary_top1000.shape,\n",
        " X_train_tf_top10.shape, X_train_tf_top100.shape, X_train_tf_top1000.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6165vpviga67",
        "outputId": "6e90f592-31d6-4183-e4c6-77941c69af7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2604, 10), (2604, 100), (2604, 1000), (2604, 10), (2604, 100), (2604, 1000))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Function to select top-N features based on chi-squared statistic\n",
        "def select_top_features_chi2(X, y, top_k):\n",
        "    selector = SelectKBest(score_func=chi2, k=top_k)\n",
        "    X_new = selector.fit_transform(X, y)\n",
        "    return X_new, selector.get_support(indices=True)\n",
        "\n",
        "# Select top-10, top-100, and top-1000 features for binary features\n",
        "X_train_binary_top10, top10_features_binary = select_top_features_chi2(X_train_binary, train_labels, 10)\n",
        "X_train_binary_top100, top100_features_binary = select_top_features_chi2(X_train_binary, train_labels, 100)\n",
        "X_train_binary_top1000, top1000_features_binary = select_top_features_chi2(X_train_binary, train_labels, 1000)\n",
        "\n",
        "# Select top-10, top-100, and top-1000 features for TF features\n",
        "X_train_tf_top10, top10_features_tf = select_top_features_chi2(X_train_tf, train_labels, 10)\n",
        "X_train_tf_top100, top100_features_tf = select_top_features_chi2(X_train_tf, train_labels, 100)\n",
        "X_train_tf_top1000, top1000_features_tf = select_top_features_chi2(X_train_tf, train_labels, 1000)\n",
        "\n",
        "(X_train_binary_top10.shape, X_train_binary_top100.shape, X_train_binary_top1000.shape,\n",
        " X_train_tf_top10.shape, X_train_tf_top100.shape, X_train_tf_top1000.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sOM9hKQgdqq",
        "outputId": "9005bc05-1750-4a09-b163-96270e0c7019"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2604, 10), (2604, 100), (2604, 1000), (2604, 10), (2604, 100), (2604, 1000))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import time\n",
        "\n",
        "# Define a function to train and evaluate a classifier\n",
        "def train_and_evaluate_classifier(clf, X_train, y_train, X_test, y_test, feature_names):\n",
        "    \"\"\"\n",
        "    Train a classifier and evaluate its performance.\n",
        "\n",
        "    Args:\n",
        "    clf: Classifier instance.\n",
        "    X_train, y_train: Training data.\n",
        "    X_test, y_test: Test data.\n",
        "    feature_names: List of feature names.\n",
        "\n",
        "    Returns:\n",
        "    dict: Evaluation results (accuracy, precision, recall, f1-score, latency, feature names).\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    latency = time.time() - start_time\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, pos_label=1, average='binary')\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'latency': latency,\n",
        "        'features': feature_names\n",
        "    }\n",
        "\n",
        "# Define a function to get feature names based on indices\n",
        "def get_feature_names(vectorizer, indices):\n",
        "    return [vectorizer.get_feature_names_out()[i] for i in indices]\n",
        "\n",
        "# Train Bernoulli Naive Bayes with binary features\n",
        "clf_bnb = BernoulliNB()\n",
        "results_bnb_top10 = train_and_evaluate_classifier(clf_bnb, X_train_binary_top10, train_labels, X_test_binary, test_labels, get_feature_names(binary_vectorizer, top10_features_binary))\n",
        "results_bnb_top100 = train_and_evaluate_classifier(clf_bnb, X_train_binary_top100, train_labels, X_test_binary, test_labels, get_feature_names(binary_vectorizer, top100_features_binary))\n",
        "results_bnb_top1000 = train_and_evaluate_classifier(clf_bnb, X_train_binary_top1000, train_labels, X_test_binary, test_labels, get_feature_names(binary_vectorizer, top1000_features_binary))\n",
        "\n",
        "# Train Multinomial Naive Bayes with binary features\n",
        "clf_mnb_binary = MultinomialNB()\n",
        "results_mnb_binary_top10 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top10, train_labels, X_test_binary, test_labels, get_feature_names(binary_vectorizer, top10_features_binary))\n",
        "results_mnb_binary_top100 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top100, train_labels, X_test_binary, test_labels, get_feature_names(binary_vectorizer, top100_features_binary))\n",
        "results_mnb_binary_top1000 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top1000, train_labels, X_test_binary, test_labels, get_feature_names(binary_vectorizer, top1000_features_binary))\n",
        "\n",
        "# Train Multinomial Naive Bayes with TF features\n",
        "clf_mnb_tf = MultinomialNB()\n",
        "results_mnb_tf_top10 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top10, train_labels, X_test_tf, test_labels, get_feature_names(tf_vectorizer, top10_features_tf))\n",
        "results_mnb_tf_top100 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top100, train_labels, X_test_tf, test_labels, get_feature_names(tf_vectorizer, top100_features_tf))\n",
        "results_mnb_tf_top1000 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top1000, train_labels, X_test_tf, test_labels, get_feature_names(tf_vectorizer, top1000_features_tf))\n",
        "\n",
        "# Display results for Bernoulli Naive Bayes with binary features\n",
        "results_bnb_top10, results_bnb_top100, results_bnb_top1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "RFq9Hbahirs_",
        "outputId": "f8c3a409-577b-4360-ef00-6d934d407945"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-846fbd5c323e>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Train Bernoulli Naive Bayes with binary features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mclf_bnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mresults_bnb_top10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_bnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_binary_top10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop10_features_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mresults_bnb_top100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_bnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_binary_top100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop100_features_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mresults_bnb_top1000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_bnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_binary_top1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop1000_features_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-846fbd5c323e>\u001b[0m in \u001b[0;36mtrain_and_evaluate_classifier\u001b[0;34m(clf, X_train, y_train, X_test, y_test, feature_names)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mlatency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m    104\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;34m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinarize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;34m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 52180 features, but BernoulliNB is expecting 10 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the same feature selection transformation to the test set\n",
        "X_test_binary_top10 = X_test_binary[:, top10_features_binary]\n",
        "X_test_binary_top100 = X_test_binary[:, top100_features_binary]\n",
        "X_test_binary_top1000 = X_test_binary[:, top1000_features_binary]\n",
        "\n",
        "X_test_tf_top10 = X_test_tf[:, top10_features_tf]\n",
        "X_test_tf_top100 = X_test_tf[:, top100_features_tf]\n",
        "X_test_tf_top1000 = X_test_tf[:, top1000_features_tf]\n",
        "\n",
        "# Retrain Bernoulli Naive Bayes with binary features\n",
        "results_bnb_top10 = train_and_evaluate_classifier(clf_bnb, X_train_binary_top10, train_labels, X_test_binary_top10, test_labels, get_feature_names(binary_vectorizer, top10_features_binary))\n",
        "results_bnb_top100 = train_and_evaluate_classifier(clf_bnb, X_train_binary_top100, train_labels, X_test_binary_top100, test_labels, get_feature_names(binary_vectorizer, top100_features_binary))\n",
        "results_bnb_top1000 = train_and_evaluate_classifier(clf_bnb, X_train_binary_top1000, train_labels, X_test_binary_top1000, test_labels, get_feature_names(binary_vectorizer, top1000_features_binary))\n",
        "\n",
        "# Retrain Multinomial Naive Bayes with binary features\n",
        "results_mnb_binary_top10 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top10, train_labels, X_test_binary_top10, test_labels, get_feature_names(binary_vectorizer, top10_features_binary))\n",
        "results_mnb_binary_top100 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top100, train_labels, X_test_binary_top100, test_labels, get_feature_names(binary_vectorizer, top100_features_binary))\n",
        "results_mnb_binary_top1000 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top1000, train_labels, X_test_binary_top1000, test_labels, get_feature_names(binary_vectorizer, top1000_features_binary))\n",
        "\n",
        "# Retrain Multinomial Naive Bayes with TF features\n",
        "results_mnb_tf_top10 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top10, train_labels, X_test_tf_top10, test_labels, get_feature_names(tf_vectorizer, top10_features_tf))\n",
        "results_mnb_tf_top100 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top100, train_labels, X_test_tf_top100, test_labels, get_feature_names(tf_vectorizer, top100_features_tf))\n",
        "results_mnb_tf_top1000 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top1000, train_labels, X_test_tf_top1000, test_labels, get_feature_names(tf_vectorizer, top1000_features_tf))\n",
        "\n",
        "# Display results for Bernoulli Naive Bayes with binary features\n",
        "results_bnb_top10, results_bnb_top100, results_bnb_top1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "o5Em2W9Qi147",
        "outputId": "3edc8544-1ce7-4d15-e45b-86dcc3abf75c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1ec254f25d59>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Retrain Multinomial Naive Bayes with binary features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mresults_mnb_binary_top10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_mnb_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_binary_top10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_binary_top10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop10_features_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mresults_mnb_binary_top100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_mnb_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_binary_top100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_binary_top100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop100_features_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mresults_mnb_binary_top1000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_mnb_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_binary_top1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_binary_top1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop1000_features_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clf_mnb_binary' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Multinomial Naive Bayes classifiers\n",
        "clf_mnb_binary = MultinomialNB()\n",
        "clf_mnb_tf = MultinomialNB()\n",
        "\n",
        "# Retrain Multinomial Naive Bayes with binary features\n",
        "results_mnb_binary_top10 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top10, train_labels, X_test_binary_top10, test_labels, get_feature_names(binary_vectorizer, top10_features_binary))\n",
        "results_mnb_binary_top100 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top100, train_labels, X_test_binary_top100, test_labels, get_feature_names(binary_vectorizer, top100_features_binary))\n",
        "results_mnb_binary_top1000 = train_and_evaluate_classifier(clf_mnb_binary, X_train_binary_top1000, train_labels, X_test_binary_top1000, test_labels, get_feature_names(binary_vectorizer, top1000_features_binary))\n",
        "\n",
        "# Retrain Multinomial Naive Bayes with TF features\n",
        "results_mnb_tf_top10 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top10, train_labels, X_test_tf_top10, test_labels, get_feature_names(tf_vectorizer, top10_features_tf))\n",
        "results_mnb_tf_top100 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top100, train_labels, X_test_tf_top100, test_labels, get_feature_names(tf_vectorizer, top100_features_tf))\n",
        "results_mnb_tf_top1000 = train_and_evaluate_classifier(clf_mnb_tf, X_train_tf_top1000, train_labels, X_test_tf_top1000, test_labels, get_feature_names(tf_vectorizer, top1000_features_tf))\n",
        "\n",
        "# Display results for Bernoulli Naive Bayes with binary features\n",
        "results_bnb_top10, results_bnb_top100, results_bnb_top1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWwdk9ZKi5xp",
        "outputId": "d5084db8-b135-4416-a967-00c761a4c526"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': 0.9106529209621993,\n",
              "  'precision': 0.7948717948717948,\n",
              "  'recall': 0.6326530612244898,\n",
              "  'f1': 0.7045454545454547,\n",
              "  'latency': 0.004006862640380859,\n",
              "  'features': ['business',\n",
              "   'click',\n",
              "   'free',\n",
              "   'income',\n",
              "   'market',\n",
              "   'million',\n",
              "   'money',\n",
              "   'remove',\n",
              "   'save',\n",
              "   'sell']},\n",
              " {'accuracy': 0.9209621993127147,\n",
              "  'precision': 0.8823529411764706,\n",
              "  'recall': 0.6122448979591837,\n",
              "  'f1': 0.7228915662650602,\n",
              "  'latency': 0.0036191940307617188,\n",
              "  'features': ['100',\n",
              "   'ad',\n",
              "   'advertise',\n",
              "   'advertisement',\n",
              "   'amaze',\n",
              "   'anywhere',\n",
              "   'aol',\n",
              "   'back',\n",
              "   'best',\n",
              "   'bonus',\n",
              "   'bulk',\n",
              "   'business',\n",
              "   'buy',\n",
              "   'cash',\n",
              "   'cd',\n",
              "   'check',\n",
              "   'click',\n",
              "   'com',\n",
              "   'company',\n",
              "   'cost',\n",
              "   'credit',\n",
              "   'customer',\n",
              "   'day',\n",
              "   'debt',\n",
              "   'dollar',\n",
              "   'dream',\n",
              "   'earn',\n",
              "   'easy',\n",
              "   'enter',\n",
              "   'ever',\n",
              "   'every',\n",
              "   'everything',\n",
              "   'fantastic',\n",
              "   'financial',\n",
              "   'free',\n",
              "   'freedom',\n",
              "   'fresh',\n",
              "   'friend',\n",
              "   'fun',\n",
              "   'guarantee',\n",
              "   'hello',\n",
              "   'here',\n",
              "   'hour',\n",
              "   'huge',\n",
              "   'hundred',\n",
              "   'income',\n",
              "   'internet',\n",
              "   'investment',\n",
              "   'language',\n",
              "   'linguistic',\n",
              "   'live',\n",
              "   'll',\n",
              "   'mailing',\n",
              "   'market',\n",
              "   'marketing',\n",
              "   'million',\n",
              "   'mlm',\n",
              "   'money',\n",
              "   'month',\n",
              "   'net',\n",
              "   'never',\n",
              "   'off',\n",
              "   'offer',\n",
              "   'online',\n",
              "   'our',\n",
              "   'over',\n",
              "   'overnight',\n",
              "   'package',\n",
              "   'pay',\n",
              "   'product',\n",
              "   'profit',\n",
              "   'profitable',\n",
              "   'purchase',\n",
              "   'remove',\n",
              "   'reply',\n",
              "   'risk',\n",
              "   'sale',\n",
              "   'save',\n",
              "   'secret',\n",
              "   'security',\n",
              "   'sell',\n",
              "   'service',\n",
              "   'ship',\n",
              "   'simply',\n",
              "   'spend',\n",
              "   'start',\n",
              "   'step',\n",
              "   'success',\n",
              "   'thousand',\n",
              "   'today',\n",
              "   'toll',\n",
              "   'university',\n",
              "   'wait',\n",
              "   'want',\n",
              "   'watch',\n",
              "   'week',\n",
              "   'win',\n",
              "   'yours',\n",
              "   'yourself',\n",
              "   'zip']},\n",
              " {'accuracy': 0.9450171821305842,\n",
              "  'precision': 1.0,\n",
              "  'recall': 0.673469387755102,\n",
              "  'f1': 0.8048780487804877,\n",
              "  'latency': 0.007846355438232422,\n",
              "  'features': ['100',\n",
              "   '10011',\n",
              "   '1302',\n",
              "   '1341',\n",
              "   '147',\n",
              "   '149',\n",
              "   '1618',\n",
              "   '195',\n",
              "   '196',\n",
              "   '199',\n",
              "   '1995',\n",
              "   '1998',\n",
              "   '200',\n",
              "   '209',\n",
              "   '24',\n",
              "   '250',\n",
              "   '300',\n",
              "   '3005',\n",
              "   '31',\n",
              "   '386',\n",
              "   '399',\n",
              "   '3d',\n",
              "   '400',\n",
              "   '409',\n",
              "   '436',\n",
              "   '470',\n",
              "   '486',\n",
              "   '50',\n",
              "   '500',\n",
              "   '550',\n",
              "   '56k',\n",
              "   '600',\n",
              "   '700',\n",
              "   '800',\n",
              "   '888',\n",
              "   '937',\n",
              "   '95',\n",
              "   '995',\n",
              "   '9am',\n",
              "   'absolutely',\n",
              "   'abstract',\n",
              "   'abuse',\n",
              "   'ac',\n",
              "   'academic',\n",
              "   'acceptance',\n",
              "   'access',\n",
              "   'accountant',\n",
              "   'accredit',\n",
              "   'accurately',\n",
              "   'acquisition',\n",
              "   'action',\n",
              "   'ad',\n",
              "   'add',\n",
              "   'addressed',\n",
              "   'addresses',\n",
              "   'adult',\n",
              "   'adults',\n",
              "   'advantage',\n",
              "   'advertise',\n",
              "   'advertisement',\n",
              "   'advertiser',\n",
              "   'affiliation',\n",
              "   'afford',\n",
              "   'affordable',\n",
              "   'again',\n",
              "   'against',\n",
              "   'ahead',\n",
              "   'aim',\n",
              "   'air',\n",
              "   'album',\n",
              "   'allow',\n",
              "   'almost',\n",
              "   'alone',\n",
              "   'alot',\n",
              "   'already',\n",
              "   'alter',\n",
              "   'alway',\n",
              "   'always',\n",
              "   'am',\n",
              "   'amateur',\n",
              "   'amaze',\n",
              "   'amazing',\n",
              "   'amazingly',\n",
              "   'amex',\n",
              "   'among',\n",
              "   'amount',\n",
              "   'analysis',\n",
              "   'anon',\n",
              "   'anything',\n",
              "   'anytime',\n",
              "   'anywhere',\n",
              "   'aol',\n",
              "   'approach',\n",
              "   'april',\n",
              "   'are',\n",
              "   'argument',\n",
              "   'asked',\n",
              "   'aspect',\n",
              "   'asset',\n",
              "   'association',\n",
              "   'assuming',\n",
              "   'astonishment',\n",
              "   'august',\n",
              "   'authenticate',\n",
              "   'author',\n",
              "   'authorization',\n",
              "   'automatically',\n",
              "   'average',\n",
              "   'away',\n",
              "   'awesome',\n",
              "   'awhile',\n",
              "   'back',\n",
              "   'bad',\n",
              "   'bank',\n",
              "   'bankruptcy',\n",
              "   'bargain',\n",
              "   'basically',\n",
              "   'batch',\n",
              "   'beach',\n",
              "   'beat',\n",
              "   'beautiful',\n",
              "   'before',\n",
              "   'beg',\n",
              "   'bel',\n",
              "   'believe',\n",
              "   'believer',\n",
              "   'below',\n",
              "   'benefits',\n",
              "   'best',\n",
              "   'bet',\n",
              "   'better',\n",
              "   'between',\n",
              "   'big',\n",
              "   'biggest',\n",
              "   'bill',\n",
              "   'billboard',\n",
              "   'billion',\n",
              "   'bin',\n",
              "   'bit',\n",
              "   'biz',\n",
              "   'blast',\n",
              "   'blockbuster',\n",
              "   'blvd',\n",
              "   'bonus',\n",
              "   'book',\n",
              "   'boss',\n",
              "   'bottom',\n",
              "   'boy',\n",
              "   'boyfriend',\n",
              "   'brand',\n",
              "   'buddy',\n",
              "   'bulk',\n",
              "   'bull',\n",
              "   'bureaus',\n",
              "   'business',\n",
              "   'button',\n",
              "   'buy',\n",
              "   'buyer',\n",
              "   'by',\n",
              "   'cable',\n",
              "   'campaign',\n",
              "   'cancun',\n",
              "   'capital',\n",
              "   'capitalfm',\n",
              "   'car',\n",
              "   'card',\n",
              "   'cards',\n",
              "   'cash',\n",
              "   'casino',\n",
              "   'catchy',\n",
              "   'cd',\n",
              "   'cds',\n",
              "   'celebrate',\n",
              "   'celebrity',\n",
              "   'cent',\n",
              "   'centre',\n",
              "   'cgi',\n",
              "   'chair',\n",
              "   'chance',\n",
              "   'channel',\n",
              "   'charge',\n",
              "   'chat',\n",
              "   'cheap',\n",
              "   'check',\n",
              "   'checks',\n",
              "   'choose',\n",
              "   'cinema',\n",
              "   'city',\n",
              "   'classified',\n",
              "   'clean',\n",
              "   'cleanest',\n",
              "   'clearance',\n",
              "   'click',\n",
              "   'client',\n",
              "   'cloak',\n",
              "   'clog',\n",
              "   'clothe',\n",
              "   'code',\n",
              "   'cognitive',\n",
              "   'colby',\n",
              "   'collector',\n",
              "   'color',\n",
              "   'com',\n",
              "   'comfort',\n",
              "   'commercialemail',\n",
              "   'commercialize',\n",
              "   'committee',\n",
              "   'company',\n",
              "   'comparative',\n",
              "   'competition',\n",
              "   'competitor',\n",
              "   'completely',\n",
              "   'compliance',\n",
              "   'comply',\n",
              "   'compuserve',\n",
              "   'computational',\n",
              "   'compzone',\n",
              "   'conceal',\n",
              "   'conference',\n",
              "   'confident',\n",
              "   'confidential',\n",
              "   'conservative',\n",
              "   'constraint',\n",
              "   'construction',\n",
              "   'consumer',\n",
              "   'context',\n",
              "   'continual',\n",
              "   'contribution',\n",
              "   'convenience',\n",
              "   'convince',\n",
              "   'copyright',\n",
              "   'corporation',\n",
              "   'corporations',\n",
              "   'corpus',\n",
              "   'correctly',\n",
              "   'cost',\n",
              "   'countless',\n",
              "   'cram',\n",
              "   'create',\n",
              "   'credit',\n",
              "   'creditor',\n",
              "   'criminal',\n",
              "   'customer',\n",
              "   'cut',\n",
              "   'cyber',\n",
              "   'daily',\n",
              "   'dare',\n",
              "   'datum',\n",
              "   'dave',\n",
              "   'david',\n",
              "   'day',\n",
              "   'days',\n",
              "   'de',\n",
              "   'deadline',\n",
              "   'debt',\n",
              "   'decide',\n",
              "   'delete',\n",
              "   'deliver',\n",
              "   'delivery',\n",
              "   'delphus',\n",
              "   'department',\n",
              "   'deposit',\n",
              "   'description',\n",
              "   'desire',\n",
              "   'desirous',\n",
              "   'desktop',\n",
              "   'development',\n",
              "   'dial',\n",
              "   'dialect',\n",
              "   'did',\n",
              "   'didn',\n",
              "   'diploma',\n",
              "   'discourse',\n",
              "   'discover',\n",
              "   'discuss',\n",
              "   'discussion',\n",
              "   'divorce',\n",
              "   'djs',\n",
              "   'doesn',\n",
              "   'dollar',\n",
              "   'dollars',\n",
              "   'don',\n",
              "   'dori',\n",
              "   'doubt',\n",
              "   'down',\n",
              "   'downline',\n",
              "   'download',\n",
              "   'downpayment',\n",
              "   'dramatically',\n",
              "   'dream',\n",
              "   'driver',\n",
              "   'drop',\n",
              "   'dupe',\n",
              "   'duplicate',\n",
              "   'each',\n",
              "   'earn',\n",
              "   'earnings',\n",
              "   'earth',\n",
              "   'ease',\n",
              "   'easiest',\n",
              "   'easily',\n",
              "   'easy',\n",
              "   'ed',\n",
              "   'edu',\n",
              "   'effective',\n",
              "   'effort',\n",
              "   'eliminate',\n",
              "   'else',\n",
              "   'emailamendtext',\n",
              "   'emailer',\n",
              "   'embark',\n",
              "   'employee',\n",
              "   'engine',\n",
              "   'english',\n",
              "   'enjoy',\n",
              "   'enter',\n",
              "   'enterprise',\n",
              "   'entertainment',\n",
              "   'entire',\n",
              "   'entrepreneur',\n",
              "   'envelope',\n",
              "   'error',\n",
              "   'esq',\n",
              "   'estate',\n",
              "   'european',\n",
              "   'evaluating',\n",
              "   'even',\n",
              "   'ever',\n",
              "   'every',\n",
              "   'everyday',\n",
              "   'everyone',\n",
              "   'everythe',\n",
              "   'everything',\n",
              "   'evidence',\n",
              "   'exactly',\n",
              "   'exceedingly',\n",
              "   'excellent',\n",
              "   'excess',\n",
              "   'excite',\n",
              "   'exclusive',\n",
              "   'exp',\n",
              "   'expensive',\n",
              "   'expiration',\n",
              "   'explode',\n",
              "   'extra',\n",
              "   'extractor',\n",
              "   'extraordinary',\n",
              "   'extremely',\n",
              "   'ez',\n",
              "   'fabulous',\n",
              "   'fairchild',\n",
              "   'faith',\n",
              "   'family',\n",
              "   'fantastic',\n",
              "   'fast',\n",
              "   'faster',\n",
              "   'fastest',\n",
              "   'favourite',\n",
              "   'fax',\n",
              "   'federal',\n",
              "   'few',\n",
              "   'fill',\n",
              "   'filled',\n",
              "   'filter',\n",
              "   'finance',\n",
              "   'financial',\n",
              "   'financially',\n",
              "   'finest',\n",
              "   'fingertip',\n",
              "   'fl',\n",
              "   'flame',\n",
              "   'flamer',\n",
              "   'fm',\n",
              "   'focus',\n",
              "   'forever',\n",
              "   'forget',\n",
              "   'formal',\n",
              "   'fortunately',\n",
              "   'fortune',\n",
              "   'fraction',\n",
              "   'framework',\n",
              "   'free',\n",
              "   'freedom',\n",
              "   'french',\n",
              "   'fresh',\n",
              "   'freshest',\n",
              "   'friend',\n",
              "   'friends',\n",
              "   'frown',\n",
              "   'fulfill',\n",
              "   'fun',\n",
              "   'future',\n",
              "   'gamble',\n",
              "   'game',\n",
              "   'gay',\n",
              "   'general',\n",
              "   'generate',\n",
              "   'genie',\n",
              "   'german',\n",
              "   'germany',\n",
              "   'gift',\n",
              "   'girdfriend',\n",
              "   'girl',\n",
              "   'girlfriend',\n",
              "   'gold',\n",
              "   'golden',\n",
              "   'goodness',\n",
              "   'goods',\n",
              "   'gov',\n",
              "   'grammar',\n",
              "   'grammatical',\n",
              "   'great',\n",
              "   'greatest',\n",
              "   'grow',\n",
              "   'grumble',\n",
              "   'guarantee',\n",
              "   'guaranteed',\n",
              "   'hand',\n",
              "   'happen',\n",
              "   'happy',\n",
              "   'hardcore',\n",
              "   'hello',\n",
              "   'help',\n",
              "   'here',\n",
              "   'hesitate',\n",
              "   'hi',\n",
              "   'historical',\n",
              "   'hit',\n",
              "   'hobby',\n",
              "   'hold',\n",
              "   'holiday',\n",
              "   'home',\n",
              "   'homeowner',\n",
              "   'homosexual',\n",
              "   'honest',\n",
              "   'hot',\n",
              "   'hotline',\n",
              "   'hotmail',\n",
              "   'hottest',\n",
              "   'hour',\n",
              "   'hours',\n",
              "   'hr',\n",
              "   'huge',\n",
              "   'hundred',\n",
              "   'hundreds',\n",
              "   'hurry',\n",
              "   'husband',\n",
              "   'id',\n",
              "   'illegal',\n",
              "   'imagination',\n",
              "   'imagine',\n",
              "   'immediate',\n",
              "   'immediately',\n",
              "   'included',\n",
              "   'income',\n",
              "   'increase',\n",
              "   'incredible',\n",
              "   'industry',\n",
              "   'inexpensive',\n",
              "   'inflation',\n",
              "   'info',\n",
              "   'infoseek',\n",
              "   'install',\n",
              "   'instant',\n",
              "   'instantly',\n",
              "   'institute',\n",
              "   'instruct',\n",
              "   'instructed',\n",
              "   'instruction',\n",
              "   'instructions',\n",
              "   'insurance',\n",
              "   'interaction',\n",
              "   'internet',\n",
              "   'interpretation',\n",
              "   'introduction',\n",
              "   'intrusion',\n",
              "   'invest',\n",
              "   'investment',\n",
              "   'investor',\n",
              "   'invite',\n",
              "   'is',\n",
              "   'isdn',\n",
              "   'isp',\n",
              "   'issue',\n",
              "   'jackson',\n",
              "   'job',\n",
              "   'john',\n",
              "   'join',\n",
              "   'jump',\n",
              "   'june',\n",
              "   'junk',\n",
              "   'juno',\n",
              "   'keep',\n",
              "   'keystroke',\n",
              "   'kid',\n",
              "   'kitchen',\n",
              "   'knock',\n",
              "   'language',\n",
              "   'lanse',\n",
              "   'largest',\n",
              "   'latest',\n",
              "   'laugh',\n",
              "   'launch',\n",
              "   'law',\n",
              "   'lawful',\n",
              "   'leave',\n",
              "   'legal',\n",
              "   'legally',\n",
              "   'legitimate',\n",
              "   'lesbian',\n",
              "   'less',\n",
              "   'let',\n",
              "   'letter',\n",
              "   'lexical',\n",
              "   'lexicon',\n",
              "   'life',\n",
              "   'lifetime',\n",
              "   'likes',\n",
              "   'limited',\n",
              "   'line',\n",
              "   'linguist',\n",
              "   'linguistic',\n",
              "   'linguistics',\n",
              "   'list',\n",
              "   'lists',\n",
              "   'literature',\n",
              "   'little',\n",
              "   'live',\n",
              "   'living',\n",
              "   'll',\n",
              "   'llc',\n",
              "   'load',\n",
              "   'loader',\n",
              "   'loan',\n",
              "   'lose',\n",
              "   'lot',\n",
              "   'lottery',\n",
              "   'love',\n",
              "   'low',\n",
              "   'luck',\n",
              "   'lucky',\n",
              "   'lyco',\n",
              "   'magazine',\n",
              "   'mailbox',\n",
              "   'mailer',\n",
              "   'mailing',\n",
              "   'make',\n",
              "   'making',\n",
              "   'manual',\n",
              "   'manufacturer',\n",
              "   'many',\n",
              "   'market',\n",
              "   'marketer',\n",
              "   'marketing',\n",
              "   'mastercard',\n",
              "   'mba',\n",
              "   'mci',\n",
              "   'mclaughlin',\n",
              "   'meg',\n",
              "   'mega',\n",
              "   'merciless',\n",
              "   'message',\n",
              "   'million',\n",
              "   'millionaire',\n",
              "   'millions',\n",
              "   'miss',\n",
              "   'mlm',\n",
              "   'model',\n",
              "   'modem',\n",
              "   'modern',\n",
              "   'moment',\n",
              "   'money',\n",
              "   'month',\n",
              "   'monthly',\n",
              "   'morphology',\n",
              "   'mortgage',\n",
              "   'most',\n",
              "   'mouse',\n",
              "   'move',\n",
              "   'movie',\n",
              "   'moving',\n",
              "   'msn',\n",
              "   'much',\n",
              "   'muncie',\n",
              "   'murkowskus',\n",
              "   'natural',\n",
              "   'nc',\n",
              "   'need',\n",
              "   'net',\n",
              "   'never',\n",
              "   'newest',\n",
              "   'news',\n",
              "   'newsgroup',\n",
              "   'newsletter',\n",
              "   'next',\n",
              "   'nl',\n",
              "   'nothing',\n",
              "   'notification',\n",
              "   'numbers',\n",
              "   'obligation',\n",
              "   'obviously',\n",
              "   'off',\n",
              "   'offer',\n",
              "   'office',\n",
              "   'offshore',\n",
              "   'once',\n",
              "   'online',\n",
              "   'operate',\n",
              "   'opportunity',\n",
              "   'order',\n",
              "   'ordering',\n",
              "   'orders',\n",
              "   'organize',\n",
              "   'originator',\n",
              "   'our',\n",
              "   'ours',\n",
              "   'over',\n",
              "   'overflow',\n",
              "   'overload',\n",
              "   'overnight',\n",
              "   'owe',\n",
              "   'own',\n",
              "   'owner',\n",
              "   'pack',\n",
              "   'package',\n",
              "   'paid',\n",
              "   'paper',\n",
              "   'papers',\n",
              "   'paradise',\n",
              "   'particular',\n",
              "   'partner',\n",
              "   'pass',\n",
              "   'password',\n",
              "   'paste',\n",
              "   'patient',\n",
              "   'pattern',\n",
              "   'pay',\n",
              "   'payable',\n",
              "   'pc',\n",
              "   'pegasus',\n",
              "   'penny',\n",
              "   'pentium',\n",
              "   'per',\n",
              "   'percent',\n",
              "   'percentage',\n",
              "   'perfectly',\n",
              "   'permanently',\n",
              "   'persistent',\n",
              "   'personal',\n",
              "   'perspective',\n",
              "   'phillip',\n",
              "   'phone',\n",
              "   'phonological',\n",
              "   'phonology',\n",
              "   'pick',\n",
              "   'piece',\n",
              "   'pile',\n",
              "   'pipeline',\n",
              "   'plans',\n",
              "   'platinum',\n",
              "   'plus',\n",
              "   'poorer',\n",
              "   'pop',\n",
              "   'porn',\n",
              "   'postage',\n",
              "   'postmaster',\n",
              "   'potential',\n",
              "   'powerful',\n",
              "   'pp',\n",
              "   'practically',\n",
              "   'pragmatic',\n",
              "   'premium',\n",
              "   'prepared',\n",
              "   'present',\n",
              "   'presentation',\n",
              "   'preview',\n",
              "   'price',\n",
              "   'print',\n",
              "   'privacy',\n",
              "   'prize',\n",
              "   'proceedings',\n",
              "   'prodigy',\n",
              "   'product',\n",
              "   'products',\n",
              "   'profanity',\n",
              "   'professional',\n",
              "   'professor',\n",
              "   'profit',\n",
              "   'profitable',\n",
              "   'profits',\n",
              "   'programme',\n",
              "   'programs',\n",
              "   'promo',\n",
              "   'promotion',\n",
              "   'promotional',\n",
              "   'prompt',\n",
              "   'promptly',\n",
              "   'proof',\n",
              "   'proposal',\n",
              "   'protect',\n",
              "   'protection',\n",
              "   'proud',\n",
              "   'prove',\n",
              "   'proven',\n",
              "   'provider',\n",
              "   'publication',\n",
              "   'publish',\n",
              "   'purchase',\n",
              "   'put',\n",
              "   'qualify',\n",
              "   'query',\n",
              "   'quick',\n",
              "   'quickly',\n",
              "   'quit',\n",
              "   'radio',\n",
              "   'raleigh',\n",
              "   'ram',\n",
              "   'rat',\n",
              "   'rate',\n",
              "   're',\n",
              "   'reach',\n",
              "   'read',\n",
              "   'real',\n",
              "   'really',\n",
              "   'reap',\n",
              "   'receive',\n",
              "   'recession',\n",
              "   'recieve',\n",
              "   'recipient',\n",
              "   'record',\n",
              "   'recruit',\n",
              "   'reference',\n",
              "   'referral',\n",
              "   'refinance',\n",
              "   'refund',\n",
              "   'reg',\n",
              "   'registration',\n",
              "   'relation',\n",
              "   'relax',\n",
              "   'release',\n",
              "   'released',\n",
              "   'relevant',\n",
              "   'remember',\n",
              "   'removal',\n",
              "   'remove',\n",
              "   'removed',\n",
              "   'rental',\n",
              "   'reply',\n",
              "   'report',\n",
              "   'reports',\n",
              "   'representation',\n",
              "   'reprinting',\n",
              "   'request',\n",
              "   'requesting',\n",
              "   'required',\n",
              "   'resale',\n",
              "   'research',\n",
              "   'researcher',\n",
              "   'resell',\n",
              "   'reselling',\n",
              "   'residual',\n",
              "   'respectability',\n",
              "   'respectfully',\n",
              "   'responsive',\n",
              "   'retail',\n",
              "   'retire',\n",
              "   'retirement',\n",
              "   'return',\n",
              "   'returns',\n",
              "   'revenue',\n",
              "   'reward',\n",
              "   'richer',\n",
              "   'right',\n",
              "   'rights',\n",
              "   'rip',\n",
              "   'risk',\n",
              "   'robbery',\n",
              "   'robbie',\n",
              "   'rockland',\n",
              "   'role',\n",
              "   'roll',\n",
              "   'rom',\n",
              "   'run',\n",
              "   'rush',\n",
              "   'sale',\n",
              "   'sales',\n",
              "   'satisfaction',\n",
              "   'satisfy',\n",
              "   'save',\n",
              "   'savings',\n",
              "   'scam',\n",
              "   'scary',\n",
              "   'science',\n",
              "   'search',\n",
              "   'secret',\n",
              "   'secrets',\n",
              "   'secure',\n",
              "   'security',\n",
              "   'sell',\n",
              "   'selle',\n",
              "   'seller',\n",
              "   'semantic',\n",
              "   'semantics',\n",
              "   'sender',\n",
              "   'sent',\n",
              "   'sentence',\n",
              "   'server',\n",
              "   'service',\n",
              "   'services',\n",
              "   'session',\n",
              "   'setup',\n",
              "   'seven',\n",
              "   'sex',\n",
              "   'sexiest',\n",
              "   'sexual',\n",
              "   'sexually',\n",
              "   'ship',\n",
              "   'shock',\n",
              "   'shop',\n",
              "   'shot',\n",
              "   'show',\n",
              "   'signature',\n",
              "   'simple',\n",
              "   'simply',\n",
              "   'sincerely',\n",
              "   'sit',\n",
              "   'site',\n",
              "   'skeptical',\n",
              "   'sleep',\n",
              "   'smart',\n",
              "   'smtp',\n",
              "   'society',\n",
              "   'sociolinguistic',\n",
              "   'software',\n",
              "   'solid',\n",
              "   'someday',\n",
              "   'someone',\n",
              "   'soon',\n",
              "   'sooner',\n",
              "   'sorry',\n",
              "   'soundblaster',\n",
              "   'sources',\n",
              "   'spam',\n",
              "   'speak',\n",
              "   'speaker',\n",
              "   'specials',\n",
              "   'speech',\n",
              "   'speed',\n",
              "   'spend',\n",
              "   'spokane',\n",
              "   'spout',\n",
              "   'staggering',\n",
              "   'stamp',\n",
              "   'stamped',\n",
              "   'star',\n",
              "   'start',\n",
              "   'started',\n",
              "   'stealth',\n",
              "   'step',\n",
              "   'steps',\n",
              "   'stock',\n",
              "   'stop',\n",
              "   'store',\n",
              "   'storm',\n",
              "   'strip',\n",
              "   'structure',\n",
              "   'student',\n",
              "   'study',\n",
              "   'stun',\n",
              "   'submission',\n",
              "   'submit',\n",
              "   'succeed',\n",
              "   'success',\n",
              "   'successful',\n",
              "   'suite',\n",
              "   'summary',\n",
              "   'super',\n",
              "   'supplies',\n",
              "   'sure',\n",
              "   'surf',\n",
              "   'sweepstakes',\n",
              "   'syntactic',\n",
              "   'syntax',\n",
              "   'tarrant',\n",
              "   'tax',\n",
              "   'technician',\n",
              "   'teen',\n",
              "   'tel',\n",
              "   'television',\n",
              "   'tell',\n",
              "   'testimonial',\n",
              "   'thank',\n",
              "   'theme',\n",
              "   'theoretical',\n",
              "   'theory',\n",
              "   'thereafter',\n",
              "   'thing',\n",
              "   'thousand',\n",
              "   'thousands',\n",
              "   'ticket',\n",
              "   'tip',\n",
              "   'tips',\n",
              "   'today',\n",
              "   'toll',\n",
              "   'top',\n",
              "   'topic',\n",
              "   'total',\n",
              "   'totally',\n",
              "   'totals',\n",
              "   'toy',\n",
              "   'trade',\n",
              "   'trail',\n",
              "   'translation',\n",
              "   'trash',\n",
              "   'tremendous',\n",
              "   'trial',\n",
              "   'trouble',\n",
              "   'true',\n",
              "   'truly',\n",
              "   'trust',\n",
              "   'try',\n",
              "   'tune',\n",
              "   'turn',\n",
              "   'tv',\n",
              "   'txt',\n",
              "   'uk',\n",
              "   'unbelievable',\n",
              "   'undeliverable',\n",
              "   'undoubtedly',\n",
              "   'unemployment',\n",
              "   'unique',\n",
              "   'university',\n",
              "   'unlimit',\n",
              "   'unlimited',\n",
              "   'unlist',\n",
              "   'unproductive',\n",
              "   'unsolicit',\n",
              "   'unsubscribe',\n",
              "   'until',\n",
              "   'unwant',\n",
              "   'upgrade',\n",
              "   'upset',\n",
              "   'us',\n",
              "   'utility',\n",
              "   'vacation',\n",
              "   'van',\n",
              "   'vanish',\n",
              "   'variety',\n",
              "   'vcs',\n",
              "   've',\n",
              "   'vendor',\n",
              "   'verb',\n",
              "   'verify',\n",
              "   'vga',\n",
              "   'video',\n",
              "   'visa',\n",
              "   'visit',\n",
              "   'vist',\n",
              "   'vulgarity',\n",
              "   'wait',\n",
              "   'wall',\n",
              "   'want',\n",
              "   'warning',\n",
              "   'warranty',\n",
              "   'waste',\n",
              "   'watch',\n",
              "   'wealth',\n",
              "   'wealthiest',\n",
              "   'wealthy',\n",
              "   'webmaster',\n",
              "   'week',\n",
              "   'weekend',\n",
              "   'weekly',\n",
              "   'weeks',\n",
              "   'welcome',\n",
              "   'whatsoever',\n",
              "   'why',\n",
              "   'wilburn',\n",
              "   'win',\n",
              "   'win95',\n",
              "   'window',\n",
              "   'winner',\n",
              "   'wisely',\n",
              "   'wish',\n",
              "   'wonderful',\n",
              "   'word',\n",
              "   'works',\n",
              "   'workshop',\n",
              "   'worldwide',\n",
              "   'worth',\n",
              "   'wrap',\n",
              "   'wrhel',\n",
              "   'xxx',\n",
              "   'yahoo',\n",
              "   'ye',\n",
              "   'yes',\n",
              "   'yours',\n",
              "   'yourself',\n",
              "   'zip',\n",
              "   'zone']})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWy6kDzCi8By"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}